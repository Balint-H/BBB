{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (5,5)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,1),(2,1),(2,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100, -100, -100, 10] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = 0\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (3,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.4, 0.2, 0.2 , 0.2]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.9, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "\n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.9, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    \n",
    "                \n",
    "                        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-471d17896928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### Question 1 : Change the policy here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4479a0879d6b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Get attributes defining the world\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsorbing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_grid_world\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Number of valid states in the gridworld (there are 22 of them)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4479a0879d6b>\u001b[0m in \u001b[0;36mbuild_grid_world\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# Get the locations of all the valid states, the neighbours of each state (by state number),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# and the absorbing states (array of 0's with ones in the absorbing states)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mlocations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsorbing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Get the number of states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4479a0879d6b>\u001b[0m in \u001b[0;36mget_topology\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# translate neighbour lists from locations to states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mnum_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mstate_neighbours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.9)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
